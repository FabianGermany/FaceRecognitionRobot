{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to demonstrate the different ways to use the MTCNN face detection module of `facenet-pytorch`. Originally reported in [Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks](https://arxiv.org/abs/1604.02878), the MTCNN network is able to simultaneously propose bounding boxes, five-point facial landmarks, and detection probabilities. Taken from the original paper:\n",
    "\n",
    "> Face detection and alignment in unconstrained environments are challenging due to various poses, illuminations and occlusions. Recent studies show that deep learning approaches can achieve impressive performance on these two tasks. In this paper, we propose a deep cascaded multi-task framework which exploits the inherent correlation between them to boost up their performance. In particular, our framework adopts a cascaded structure with three stages of carefully designed deep convolutional networks that predict face and landmark location in a coarse-to-fine manner. In addition, in the learning process, we propose a new online hard sample mining strategy that can improve the performance automatically without manual sample selection. Our method achieves superior accuracy over the state-of-the-art techniques on the challenging FDDB and WIDER FACE benchmark for face detection, and AFLW benchmark for face alignment, while keeps real time performance.\n",
    "\n",
    "`facenet-pytorch` includes an efficient, cuda-ready implementation of MTCNN that will be demonstrated in this notebook. The following topics will be covered:\n",
    "\n",
    "1. <a href='#1'>Documentation</a>\n",
    "1. <a href='#2'>Basic usage</a>\n",
    "1. <a href='#3'>Preventing image normalization</a>\n",
    "1. <a href='#4'>Margin adjustment</a>\n",
    "1. <a href='#5'>Multiple faces in a single image</a>\n",
    "1. <a href='#6'>Batched detection</a>\n",
    "1. <a href='#7'>Bounding boxes and facial landmarks</a>\n",
    "1. <a href='#8'>Saving face datasets</a>\n",
    "\n",
    "Other resources:\n",
    "\n",
    "1. The facenet-pytorch [github repo](https://github.com/timesler/facenet-pytorch)\n",
    "1. [Notebook demonstrating combined use of face detection and recognition](https://www.kaggle.com/timesler/facial-recognition-model-in-pytorch)\n",
    "1. [The FastMTCNN algorithm](https://www.kaggle.com/timesler/fast-mtcnn-detector-45-fps-at-full-resolution) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install /kaggle/input/facenet-pytorch-vggface2/facenet_pytorch-2.2.7-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import MTCNN\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='1'>Documentation</a>\n",
    "\n",
    "Detailed usage information is contained in the MTCNN docstring:\n",
    "\n",
    "```\n",
    "help(MTCNN)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='2'>Basic usage</a>\n",
    "\n",
    "Unlike other implementations, calling a `facenet-pytorch` MTCNN object directly with an image (i.e., using the forward method for those familiar with pytorch) will return torch tensors containing the detected face(s), rather than just the bounding boxes. This is to enable using the module easily as the first stage of a facial recognition pipeline, in which the faces are passed directly to an additional network or algorithm.\n",
    "\n",
    "In order to return the detected boxes instead (and optionally, the facial landmarks), see the `MTCNN.detect()` method. Its use will be described below also.\n",
    "\n",
    "To create an MTCNN detector that runs on the GPU, instantiate the model with `device='cuda:0'` or equivalent.\n",
    "\n",
    "For this competition, it will be best to set `select_largest=False` to ensure detected faces are ordered according to detection probability rather than size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create face detector\n",
    "mtcnn = MTCNN(select_largest=False, device='cuda')\n",
    "\n",
    "# Load a single image and display\n",
    "v_cap = cv2.VideoCapture('/kaggle/input/deepfake-detection-challenge/train_sample_videos/agqphdxmwt.mp4')\n",
    "success, frame = v_cap.read()\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "frame = Image.fromarray(frame)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(frame)\n",
    "plt.axis('off')\n",
    "\n",
    "# Detect face\n",
    "face = mtcnn(frame)\n",
    "face.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='3'>Preventing image normalization</a>\n",
    "\n",
    "By default, the MTCNN module of `facenet-pytorch` applies fixed image standardization to faces before returning so they are well suited for the package's face recognition model.\n",
    "\n",
    "If you want to get out images that look more normal to the human eye, this normalization can be prevented by creating the detector with `post_process=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create face detector\n",
    "mtcnn = MTCNN(select_largest=False, post_process=False, device='cuda:0')\n",
    "\n",
    "# Detect face\n",
    "face = mtcnn(frame)\n",
    "\n",
    "# Visualize\n",
    "plt.imshow(face.permute(1, 2, 0).int().numpy())\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='4'>Margin adjustment</a>\n",
    "\n",
    "Depending on your downstream processing and how fakes can be identified, you may want to add more (or less) of a margin around the detected faces. This is controlled using the `margin` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create face detector\n",
    "mtcnn = MTCNN(margin=40, select_largest=False, post_process=False, device='cuda:0')\n",
    "\n",
    "# Detect face\n",
    "face = mtcnn(frame)\n",
    "\n",
    "# Visualize\n",
    "plt.imshow(face.permute(1, 2, 0).int().numpy())\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='5'>Multiple faces in a single image</a>\n",
    "\n",
    "Using MTCNN as above will only return a single face from each frame (or None if none are detected). Since some of the videos in the dataset contain more than one face, you will likely want to return all detected faces as any/all of them may have been modified. This is acheived by setting `keep_all=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create face detector\n",
    "mtcnn = MTCNN(margin=20, keep_all=True, post_process=False, device='cuda:0')\n",
    "\n",
    "# Load a single image and display\n",
    "v_cap = cv2.VideoCapture('/kaggle/input/deepfake-detection-challenge/train_sample_videos/avibnnhwhp.mp4')\n",
    "success, frame = v_cap.read()\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "frame = Image.fromarray(frame)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(frame)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Detect face\n",
    "faces = mtcnn(frame)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, len(faces))\n",
    "for face, ax in zip(faces, axes):\n",
    "    ax.imshow(face.permute(1, 2, 0).int().numpy())\n",
    "    ax.axis('off')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='6'>Batched detection</a>\n",
    "\n",
    "`facenet-pytorch` is also capable of performing face detection on batches of images, typically providing considerable speed-up. A batch should be structured as list of PIL images of equal dimension. The returned object will have an additional first dimension corresponding to the batch. Each image in the batch may have one or more faces detected.\n",
    "\n",
    "In the following example, we use MTCNN to detect multiple faces in:\n",
    "1. A single batch of frames, and\n",
    "1. Every frame of a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create face detector\n",
    "mtcnn = MTCNN(margin=20, keep_all=True, post_process=False, device='cuda:0')\n",
    "\n",
    "# Load a video\n",
    "v_cap = cv2.VideoCapture('/kaggle/input/deepfake-detection-challenge/train_sample_videos/avibnnhwhp.mp4')\n",
    "v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Loop through video, taking a handful of frames to form a batch\n",
    "frames = []\n",
    "for i in tqdm(range(v_len)):\n",
    "    \n",
    "    # Load frame\n",
    "    success = v_cap.grab()\n",
    "    if i % 50 == 0:\n",
    "        success, frame = v_cap.retrieve()\n",
    "    else:\n",
    "        continue\n",
    "    if not success:\n",
    "        continue\n",
    "        \n",
    "    # Add to batch\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frames.append(Image.fromarray(frame))\n",
    "\n",
    "# Detect faces in batch\n",
    "faces = mtcnn(frames)\n",
    "\n",
    "fig, axes = plt.subplots(len(faces), 2, figsize=(6, 15))\n",
    "for i, frame_faces in enumerate(faces):\n",
    "    for j, face in enumerate(frame_faces):\n",
    "        axes[i, j].imshow(face.permute(1, 2, 0).int().numpy())\n",
    "        axes[i, j].axis('off')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example uses a similar approach to detect all faces in all frames in a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a video\n",
    "v_cap = cv2.VideoCapture('/kaggle/input/deepfake-detection-challenge/train_sample_videos/avibnnhwhp.mp4')\n",
    "v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "# Loop through video\n",
    "batch_size = 16\n",
    "frames = []\n",
    "faces = []\n",
    "for _ in tqdm(range(v_len)):\n",
    "    \n",
    "    # Load frame\n",
    "    success, frame = v_cap.read()\n",
    "    if not success:\n",
    "        continue\n",
    "        \n",
    "    # Add to batch\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frames.append(Image.fromarray(frame))\n",
    "    \n",
    "    # When batch is full, detect faces and reset batch list\n",
    "    if len(frames) >= batch_size:\n",
    "        faces.extend(mtcnn(frames))\n",
    "        frames = []\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot([len(f) for f in faces])\n",
    "plt.title('Detected faces per frame');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='7'>Bounding boxes and facial landmarks</a>\n",
    "\n",
    "To return bounding boxes and facial landmarks from MTCNN, instead of calling the `mtcnn` object directly, call `mtcnn.detect()` instead.\n",
    "\n",
    "Unlike the forward method (shown in each of examples above), the `.detect()` method will always return all detected bounding boxes (and optional landmarks) in an image.\n",
    "\n",
    "The following example demonstrates the use of the `.detect()` method on a single image.\n",
    "\n",
    "Note that the `margin` argument, if used when creating the MTCNN detector, is not used in the `detect()` method. `detect()` returns the true bounding boxes, so the margin can be applied subsequently by the user if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-82f210b3a559>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create face detector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmtcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMTCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load a single image and display\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mv_cap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoCapture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "# Create face detector\n",
    "mtcnn = MTCNN(keep_all=True, device='cuda:0')\n",
    "\n",
    "# Load a single image and display\n",
    "v_cap = cv2.VideoCapture(0)\n",
    "success, frame = v_cap.read()\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "frame = Image.fromarray(frame)\n",
    "\n",
    "# Detect face\n",
    "boxes, probs, landmarks = mtcnn.detect(frame, landmarks=True)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "ax.imshow(frame)\n",
    "ax.axis('off')\n",
    "\n",
    "for box, landmark in zip(boxes, landmarks):\n",
    "    ax.scatter(*np.meshgrid(box[[0, 2]], box[[1, 3]]))\n",
    "    ax.scatter(landmark[:, 0], landmark[:, 1], s=8)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example demonstrates how to show bounding boxes and facial landmarks in every frame in a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a video\n",
    "v_cap = cv2.VideoCapture('/kaggle/input/deepfake-detection-challenge/train_sample_videos/avibnnhwhp.mp4')\n",
    "\n",
    "# Loop through video\n",
    "batch_size = 32\n",
    "frames = []\n",
    "boxes = []\n",
    "landmarks = []\n",
    "view_frames = []\n",
    "view_boxes = []\n",
    "view_landmarks = []\n",
    "for _ in tqdm(range(v_len)):\n",
    "    \n",
    "    # Load frame\n",
    "    success, frame = v_cap.read()\n",
    "    if not success:\n",
    "        continue\n",
    "        \n",
    "    # Add to batch, resizing for speed\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = Image.fromarray(frame)\n",
    "    frame = frame.resize([int(f * 0.25) for f in frame.size])\n",
    "    frames.append(frame)\n",
    "    \n",
    "    # When batch is full, detect faces and reset batch list\n",
    "    if len(frames) >= batch_size:\n",
    "        batch_boxes, _, batch_landmarks = mtcnn.detect(frames, landmarks=True)\n",
    "        boxes.extend(batch_boxes)\n",
    "        landmarks.extend(batch_landmarks)\n",
    "        \n",
    "        view_frames.append(frames[-1])\n",
    "        view_boxes.append(boxes[-1])\n",
    "        view_landmarks.append(landmarks[-1])\n",
    "        \n",
    "        frames = []\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(3, 3, figsize=(18, 12))\n",
    "for i in range(9):\n",
    "    ax[int(i / 3), i % 3].imshow(view_frames[i])\n",
    "    ax[int(i / 3), i % 3].axis('off')\n",
    "    for box, landmark in zip(view_boxes[i], view_landmarks[i]):\n",
    "        ax[int(i / 3), i % 3].scatter(*np.meshgrid(box[[0, 2]], box[[1, 3]]), s=8)\n",
    "        ax[int(i / 3), i % 3].scatter(landmark[:, 0], landmark[:, 1], s=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='8'>Saving face datasets</a>\n",
    "\n",
    "In order to save detected faces directly to file, use MTCNN's `save_path` argument in the forward function. This is compatible with both single images and batch processing.\n",
    "\n",
    "- For single images, pass a single path string (e.g., '{videoname}\\_{frame}.jpg')\n",
    "- For batches of images, pass a list of path strings (one for each frame)\n",
    "\n",
    "When multiple faces are detected in a single image, additional faces are each saved with an incremental integer appended to the end of the save path (e.g., '{videoname}\\_{frame}.jpg' and '{videoname}\\_{frame}\\_1.jpg')\n",
    "\n",
    "See example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single image\n",
    "mtcnn(frame, save_path='single_image.jpg');\n",
    "\n",
    "# Batch\n",
    "save_paths = [f'image_{i}.jpg' for i in range(len(frames))]\n",
    "mtcnn(frames, save_path=save_paths);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
